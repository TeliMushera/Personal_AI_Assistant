# -*- coding: utf-8 -*-
"""Personal_AI_Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PO05ZaCY-6__w15OyyHrYzVjgBZcnpAK
"""

!pip install transformers torch accelerate -q

from transformers import pipeline
assistant=pipeline('text2text-generation',model='google/flan-t5-large')
#load and install open-source model
#pipeline is a library

"""building a chat function"""

chat_history=[]
def ask_ai(user_input):
  global chat_history
  chat_history.append(f'User: {user_input}')
  context='\n'.join(chat_history[-5:])
  response=assistant(context)[0]['generated_text']
  chat_history.append(f'assistant:{response}')
  return response #we use return for better optimization

while True:
  user_input=input('You: ')
  if user_input.lower() in ['exit','bye','quit']:
    print('assistant goodbye!')
    break
  reply=ask_ai(user_input)
  print(f'assisntant: {reply}')